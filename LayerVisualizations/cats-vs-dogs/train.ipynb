{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gbwF0CWA1hVA"
   },
   "source": [
    "Below we're using Keras, to train an NN with images of cats and dogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3212
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 427568,
     "status": "ok",
     "timestamp": 1555295748894,
     "user": {
      "displayName": "Vivek Pradeep Tiwari",
      "photoUrl": "",
      "userId": "02063672579948753737"
     },
     "user_tz": 420
    },
    "id": "vZ6mlChb1fsI",
    "outputId": "15567343-a123-4765-edc4-37a611e4ba49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_60 (Conv2D)           (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_61 (Conv2D)           (None, 72, 72, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_37 (MaxPooling (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_62 (Conv2D)           (None, 34, 34, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_38 (MaxPooling (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_63 (Conv2D)           (None, 15, 15, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_39 (MaxPooling (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "lambda_14 (Lambda)           (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 65,698\n",
      "Trainable params: 65,698\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 804 images belonging to 2 classes.\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 66s 440ms/step - loss: 0.2182 - accuracy: 0.9144 - val_loss: 0.5205 - val_accuracy: 0.8288\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 36s 285ms/step - loss: 0.1697 - accuracy: 0.9289 - val_loss: 0.6431 - val_accuracy: 0.8363\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 34s 275ms/step - loss: 0.1956 - accuracy: 0.9344 - val_loss: 0.6316 - val_accuracy: 0.8288\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 36s 283ms/step - loss: 0.1522 - accuracy: 0.9373 - val_loss: 0.8924 - val_accuracy: 0.8100\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 35s 279ms/step - loss: 0.1769 - accuracy: 0.9360 - val_loss: 0.5232 - val_accuracy: 0.8250\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 33s 266ms/step - loss: 0.1411 - accuracy: 0.9386 - val_loss: 0.6138 - val_accuracy: 0.8313\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 31s 251ms/step - loss: 0.1436 - accuracy: 0.9405 - val_loss: 0.5608 - val_accuracy: 0.8250\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 32s 254ms/step - loss: 0.1576 - accuracy: 0.9424 - val_loss: 0.5814 - val_accuracy: 0.8350\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 32s 253ms/step - loss: 0.1375 - accuracy: 0.9489 - val_loss: 0.5663 - val_accuracy: 0.8313\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 34s 274ms/step - loss: 0.1759 - accuracy: 0.9279 - val_loss: 0.6399 - val_accuracy: 0.7950\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - 35s 277ms/step - loss: 0.1849 - accuracy: 0.9199 - val_loss: 0.8963 - val_accuracy: 0.7900\n",
      "Epoch 12/50\n",
      "125/125 [==============================] - 36s 284ms/step - loss: 0.1557 - accuracy: 0.9414 - val_loss: 0.5088 - val_accuracy: 0.8225\n",
      "Epoch 13/50\n",
      "125/125 [==============================] - 35s 282ms/step - loss: 0.1527 - accuracy: 0.9334 - val_loss: 0.6681 - val_accuracy: 0.8338\n",
      "Epoch 14/50\n",
      "125/125 [==============================] - 35s 282ms/step - loss: 0.1540 - accuracy: 0.9442 - val_loss: 0.6633 - val_accuracy: 0.8313\n",
      "Epoch 15/50\n",
      "125/125 [==============================] - 35s 282ms/step - loss: 0.1400 - accuracy: 0.9485 - val_loss: 0.6555 - val_accuracy: 0.8313\n",
      "Epoch 16/50\n",
      "125/125 [==============================] - 35s 282ms/step - loss: 0.1417 - accuracy: 0.9417 - val_loss: 0.6123 - val_accuracy: 0.8275\n",
      "Epoch 17/50\n",
      "125/125 [==============================] - 35s 280ms/step - loss: 0.1426 - accuracy: 0.9487 - val_loss: 0.5149 - val_accuracy: 0.8200\n",
      "Epoch 18/50\n",
      "125/125 [==============================] - 35s 282ms/step - loss: 0.1600 - accuracy: 0.9397 - val_loss: 0.7781 - val_accuracy: 0.8263\n",
      "Epoch 19/50\n",
      "125/125 [==============================] - 35s 282ms/step - loss: 0.1476 - accuracy: 0.9470 - val_loss: 0.5320 - val_accuracy: 0.8550\n",
      "Epoch 20/50\n",
      "125/125 [==============================] - 36s 286ms/step - loss: 0.1552 - accuracy: 0.9377 - val_loss: 0.7285 - val_accuracy: 0.8462\n",
      "Epoch 21/50\n",
      "125/125 [==============================] - 35s 282ms/step - loss: 0.1418 - accuracy: 0.9515 - val_loss: 0.6357 - val_accuracy: 0.8388\n",
      "Epoch 22/50\n",
      "125/125 [==============================] - 35s 283ms/step - loss: 0.1293 - accuracy: 0.9555 - val_loss: 0.9000 - val_accuracy: 0.7887\n",
      "Epoch 23/50\n",
      "125/125 [==============================] - 52s 415ms/step - loss: 0.1317 - accuracy: 0.9520 - val_loss: 0.5307 - val_accuracy: 0.8350\n",
      "Epoch 24/50\n",
      "125/125 [==============================] - 36s 282ms/step - loss: 0.1583 - accuracy: 0.9329 - val_loss: 0.6840 - val_accuracy: 0.8250\n",
      "Epoch 25/50\n",
      "125/125 [==============================] - 55s 444ms/step - loss: 0.1029 - accuracy: 0.9650 - val_loss: 0.6811 - val_accuracy: 0.8400\n",
      "Epoch 26/50\n",
      "125/125 [==============================] - 45s 357ms/step - loss: 0.1548 - accuracy: 0.9453 - val_loss: 0.8379 - val_accuracy: 0.7763\n",
      "Epoch 27/50\n",
      "125/125 [==============================] - 36s 290ms/step - loss: 0.1506 - accuracy: 0.9450 - val_loss: 1.9888 - val_accuracy: 0.7300\n",
      "Epoch 28/50\n",
      "125/125 [==============================] - 39s 309ms/step - loss: 0.2079 - accuracy: 0.9387 - val_loss: 0.5417 - val_accuracy: 0.8462\n",
      "Epoch 29/50\n",
      "125/125 [==============================] - 35s 279ms/step - loss: 0.1554 - accuracy: 0.9381 - val_loss: 0.6477 - val_accuracy: 0.8350\n",
      "Epoch 30/50\n",
      "125/125 [==============================] - 35s 281ms/step - loss: 0.1284 - accuracy: 0.9525 - val_loss: 0.6751 - val_accuracy: 0.8350\n",
      "Epoch 31/50\n",
      "125/125 [==============================] - 35s 278ms/step - loss: 0.1239 - accuracy: 0.9609 - val_loss: 0.9949 - val_accuracy: 0.7925\n",
      "Epoch 32/50\n",
      "125/125 [==============================] - 35s 281ms/step - loss: 0.1205 - accuracy: 0.9526 - val_loss: 0.8095 - val_accuracy: 0.8338\n",
      "Epoch 33/50\n",
      "125/125 [==============================] - 35s 279ms/step - loss: 0.1355 - accuracy: 0.9558 - val_loss: 0.7315 - val_accuracy: 0.8325\n",
      "Epoch 34/50\n",
      "125/125 [==============================] - 35s 277ms/step - loss: 0.1408 - accuracy: 0.9501 - val_loss: 0.5568 - val_accuracy: 0.8150\n",
      "Epoch 35/50\n",
      "125/125 [==============================] - 35s 279ms/step - loss: 0.1124 - accuracy: 0.9599 - val_loss: 0.7041 - val_accuracy: 0.8400\n",
      "Epoch 36/50\n",
      "125/125 [==============================] - 35s 282ms/step - loss: 0.1037 - accuracy: 0.9575 - val_loss: 0.7439 - val_accuracy: 0.8388\n",
      "Epoch 37/50\n",
      "125/125 [==============================] - 35s 282ms/step - loss: 0.1240 - accuracy: 0.9540 - val_loss: 1.4376 - val_accuracy: 0.7725\n",
      "Epoch 38/50\n",
      "125/125 [==============================] - 35s 281ms/step - loss: 0.1522 - accuracy: 0.9484 - val_loss: 0.6604 - val_accuracy: 0.8400\n",
      "Epoch 39/50\n",
      "125/125 [==============================] - 35s 282ms/step - loss: 0.1365 - accuracy: 0.9482 - val_loss: 1.1238 - val_accuracy: 0.7412\n",
      "Epoch 40/50\n",
      "125/125 [==============================] - 35s 281ms/step - loss: 0.1326 - accuracy: 0.9481 - val_loss: 0.6861 - val_accuracy: 0.8487\n",
      "Epoch 41/50\n",
      "125/125 [==============================] - 35s 277ms/step - loss: 0.1495 - accuracy: 0.9551 - val_loss: 1.3775 - val_accuracy: 0.7713\n",
      "Epoch 42/50\n",
      "125/125 [==============================] - 37s 298ms/step - loss: 0.1588 - accuracy: 0.9309 - val_loss: 0.7285 - val_accuracy: 0.8238\n",
      "Epoch 43/50\n",
      "125/125 [==============================] - 49s 388ms/step - loss: 0.1301 - accuracy: 0.9540 - val_loss: 0.5980 - val_accuracy: 0.8325\n",
      "Epoch 44/50\n",
      "125/125 [==============================] - 40s 322ms/step - loss: 0.1138 - accuracy: 0.9574 - val_loss: 0.7651 - val_accuracy: 0.8363\n",
      "Epoch 45/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 36s 285ms/step - loss: 0.1375 - accuracy: 0.9495 - val_loss: 0.7039 - val_accuracy: 0.8200\n",
      "Epoch 46/50\n",
      "125/125 [==============================] - 35s 278ms/step - loss: 0.1231 - accuracy: 0.9561 - val_loss: 1.5849 - val_accuracy: 0.7900\n",
      "Epoch 47/50\n",
      "125/125 [==============================] - 31s 249ms/step - loss: 0.1640 - accuracy: 0.9501 - val_loss: 0.6224 - val_accuracy: 0.8363\n",
      "Epoch 48/50\n",
      "125/125 [==============================] - 31s 245ms/step - loss: 0.1011 - accuracy: 0.9624 - val_loss: 0.7145 - val_accuracy: 0.8438\n",
      "Epoch 49/50\n",
      "125/125 [==============================] - 31s 247ms/step - loss: 0.1186 - accuracy: 0.9519 - val_loss: 0.8333 - val_accuracy: 0.8350\n",
      "Epoch 50/50\n",
      "125/125 [==============================] - 31s 246ms/step - loss: 0.1151 - accuracy: 0.9574 - val_loss: 0.7599 - val_accuracy: 0.8413\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7682fe6160>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Dense, Lambda\n",
    "from keras import backend as K\n",
    "from keras.callbacks import *\n",
    "\n",
    "\n",
    "def global_average_pooling(x):\n",
    "    return K.mean(x, axis = (1, 2))\n",
    "\n",
    "def global_average_pooling_shape(input_shape):\n",
    "    return input_shape[0:2]\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'\n",
    "nb_train_samples = 2000\n",
    "nb_validation_samples = 800\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape,activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Lambda(global_average_pooling, \n",
    "              output_shape=global_average_pooling_shape))\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.load_weights(\"weights/weights.50-0.47.h5\")\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "checkpoint_path=\"weights/weights.{epoch:02d}-{val_loss:.2f}.h5\"\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss',\n",
    "                             verbose=0, save_best_only=False, save_weights_only=False, mode='auto')\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    )\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size,\n",
    "    callbacks=checkpoint)\n",
    "\n",
    "# model.save_weights('weights.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VBfCNs9q4I48"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "train.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
