{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fee83f2",
   "metadata": {},
   "source": [
    "## Why am I writing this?\n",
    "\n",
    "Well I was revising concepts in Machine Learning. And if you have ever done so yourself, you might agree it won't go without taking a dive into PCA, why  is it, what is it and how is it used. \n",
    "\n",
    "### Let's first talk about Why PCA?\n",
    "\n",
    "Consider a rock band that is made up of 20 members. That's too many members for a band isn't it. Maybe yes, maybe no. Let's say the band has 3 guitarists, couple of vocalists, one on piano, one on an electronic keyboard, some have flute etc. The band size might work okay depending on the venue like Madison-square or Wembley. But what if the only available venues for this band are coffee shops. Well in that case, 20 members just might seem too many. What seems appropriate for the coffee shops is having like 1 vocalist, 1 on drums, 1 on guitar and may be 1 can play keyboard. You might not get all the intricate details of a song but it just might be good enough for the Coffee shop to listen to in an almos MTV Unplugged kind od music. \n",
    "\n",
    "This is exactly the purpose of PCA (Principal Component Analysis) where the band members are the variables representing the data and the song is what the data is representing.\n",
    "\n",
    "#### With PCA, we are trying to find an alternate representation of data with lesser number of variables such that we are able to capture maximum variance in the data.\n",
    "\n",
    "\n",
    "### What is PCA?\n",
    "\n",
    "PCA reduces the dimensionality of input by combining the input variable in such a way that it maximizes variance. Essentially, it is looking to project inputs along an axis maximizing variance. Okay, I've mentioned this multiple times now that we are looking to maximize variance but what does that mean. In simple terms, we are looking to capture most information with lesser variable. Before diving deeper, it is important to have the inputs scaled, and by scaling I mean to subract mean from the input and dividing by standard deviation. As we follow on the math, it would start making sense why that is important. \n",
    "\n",
    "So let get into this now.\n",
    "Consider X be the input matrix of n samples and m variables (nxm). \n",
    "We are looking to find a representation of the inputs t such that \n",
    "\n",
    "max var(t), S.T (Wtrans)xW = 1\n",
    "\n",
    "where t = $\\sum$ XW  over all samples\n",
    "\n",
    "X = input matrix\n",
    "W = weight vector\n",
    "t = scores vector (this will provide scores to each of the variables in the input)\n",
    "\n",
    "max $\\sum$ ((t - tmean)(t-tmean))/(n-1)\n",
    "\n",
    "Now since, we have scaled the data so the t has 0 mean and unit variance, this expression just reduces to\n",
    "\n",
    "max t.t\n",
    "\n",
    "using the definition of t we have\n",
    "\n",
    "max (Wtrans.X)(Xtrans.W)\n",
    "\n",
    "S.T WtransW = 1\n",
    "\n",
    "We have something golden here. Let's see what is it and how. We can apply Lagrange's multiplier here to get this into an optimization problem to solve.\n",
    "\n",
    "L = F - pG\n",
    "p = Lagrange multipler\n",
    "G = WtransW\n",
    "\n",
    "We want to maximize\n",
    "\n",
    "L = (Wtrans.Xtrans)(X.W) - p(WtransW)\n",
    "\n",
    "dL/dW = 0 and dL/dX = 0\n",
    "\n",
    "dL/dW = XtransXW - pW = 0\n",
    "\n",
    "dL/dX = WtransW = 0\n",
    "\n",
    "\n",
    "This gives us,\n",
    "\n",
    "(XtransX)W = pW ....we found something golden!!!\n",
    "\n",
    "This essentially reduces to the Eigenvalue problem. If you are not aware of Eigen values and Eigen vectors, If I have to simply put for a matrix A that is invertible and eigen vector v with the eigen value p relates,\n",
    "Av = pv\n",
    "\n",
    "Since we autoscaled our data, XtransX is simply the covariance matrix of X, which has two good properties \n",
    "1. It is symmetric\n",
    "2. It is positive definite\n",
    "\n",
    "Solving for eigen value, the largest eigen value gives us the optimal weight for a single principle component. This naturally extends to multiple principle components so as to why stop at the largest eigen value, and we can go on to have k number of principle components in that manner.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
